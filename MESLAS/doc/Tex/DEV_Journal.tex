\documentclass[a4paper,10pt]{article}
\usepackage[utf8x]{inputenc}
\usepackage{amssymb,amsmath,amsthm}
\usepackage{hyperref}
\usepackage{cleveref}

% For Python syntax highlighting.
\usepackage{minted}

\usepackage[margin=1.0in]{geometry}


% For testing wether an argument of a macro is empty.
\usepackage{xifthen}
\usepackage{bm}

% Figures and subfigures.
\usepackage{subfig}
\usepackage{graphicx}

\newcommand{\dl}{\mathrm{d}\lambda}
\setlength{\parindent}{0cm}

%opening
\title{Development Journal and Specs of the MESLAS package
}

% \input{macros}
\input{macros}

\begin{document}
\maketitle

\section{General Considerations}
The MESLAS package (\textbf{M}ulti-variate \textbf{E}xcursion \textbf{S}et \textbf{L}earning by \textbf{A}daptive \textbf{S}ampling) is a toolbox for simulation and prediction of mulitivariate Gaussian random fields.\\

The setup of the package is the following: $\gp$ is a $\no$-dimensional random
field on a domain $\nd$-dimensional
domain $D$.

Our philosophy is to always specify spatial location and response indices
together. That is, one should always specify \textbf{where} and \textbf{what}.

Spatial locations are denoted by $s$ and response indices by $\ell$. We will
use boldface (or, in the code, alternatively uppercase or plurals) to denote
vectors of such objects.

\medskip
A generalized sampling location is thus entirely defined by specifying two vectors
\begin{align*}
    \bm{s} &= \left(s_1, ..., s_n\right)\in D^n\\
    \bm{\ell} &= \left(\ell_1, ..., \ell_n\right) \in \lbrace 1, ..., \no
    \rbrace^n
\end{align*}
We will refer to $n$ as the \textit{dimension} of the generalized sampling
location and usually just talk of location, using the word \textit{spatial
location} when we want to specifically refer to points in $D$. Also, we will
use boldface $x$ as a shortcut to refer to the couple $\left(\bm{s},
\bm{\ell}\right)$ of spatial location vector and response index vector.
The shortcut notation $\gp[\bm{x}]$ thus refers to the
vector
\[
    \gp[\bm{x}]:=\left(\gp[s_1]^{\ell_1}, ..., \gp[s_n]^{\ell_n}\right) \in
    \mathbb{R}^n.
\]

\subsection{Covariance Model}
We assume a factor model which is the product of a stationary spatial component
with a response-index component
\begin{equation}
    \textrm{Cov}\left(\gp[s]^i, \gp[t]^j\right) = k\left(s - t\right)
    \gamma\left(i, j\right).
\end{equation}

This makes implementation easier, since then, to compute the covariance matrix
of a generalized observations $\left(S, L\right)$, we first compute the
pairwise distance matrix
\[
    H = \textrm{cdist}\left(S,S, p=2\right) = \begin{pmatrix}
        ||s_1 - s_1|| & \dots & ||s_1 - s_n||\\
        \vdots &  & \vdots \\
        ||s_n - s_1|| & \dots & ||s_n - s_n||\\
    \end{pmatrix}
\]
which can then be feeded to a vectorized stationary covariance function to get
$K(H)$.

For the response index part, we compute $L_1, L_2=\textrm{meshgrid}\left(L,
L\right)$ which yields
\begin{align*}
    L_1 =  \begin{pmatrix}
        l_1 & \dots & l_1\\
        \vdots &  & \vdots \\
        l_n & \dots & l_n\\
    \end{pmatrix}
    &,~ 
    L_2 =  \begin{pmatrix}
        l_1 & \dots & l_n\\
        \vdots &  & \vdots \\
        l_1 & \dots & l_n\\
    \end{pmatrix}
\end{align*}
and then feed it to a vectorized cross-covariance function $\gamma(L_1, L_2)$.
Finally, we get the covariance matrix by elementwise multiplication
\[
    K = K\left(H\right) \odot \gamma\left(L_1, L_2\right)
\]

\subsection{Cross-Covariance Models}
We here review different usual models for the cross-covariance part $\gamma(.,
.)$ of the covariance function. Recall that this is the part that specifies how
different components of the response vector at one fixed location interact.

\medskip
The simplest model we will consider is \textbf{uniform mixing}. In this model,
all components interact with the same coupling $\gamma_0$:
\begin{equation}
    \gamma(l, m) = \begin{cases} \sigma_l^2,~ l=m\\ 
        \gamma_0\sigma_l\sigma_m,~l\neq m
    \end{cases}
\end{equation}
and $\sigma_1^2, ..., \sigma_{\no}^2$ are the variances of the individual
components.

\subsection{Implementation Details}
ATTENTION: torch.meshgrid behaves differently than numpy's one.
First of all, it takes single dimensional vectors.
\[
    L=(1,2,3,4),~ \textrm{torch.meshgrid}(L,L)=
    \begin{pmatrix}
        1 & \dots & 1\\
        \vdots &  & \vdots \\
        n & \dots & n\\
    \end{pmatrix}
    ,~ 
    \begin{pmatrix}
        1 & \dots & n\\
        \vdots &  & \vdots \\
        1 & \dots & n\\
    \end{pmatrix}
\]

\subsection{Mean Module}
\subsection{Covariance Module}
\subsection{Gaussian Random Field Class and Sampling}
\subsection{Gridding}

\newpage

\section{Example Run}
We consider a $2$-dimensional GRF on a a $2$-dimensional $100\times 100$ regular grid on
$[0,1]^2$. The GRF has a factor covariance model, where the spatial part is a
Mat\'{e}rn $3/2$ with unit variance and lengthscale $\lambda_0 = 0.1$. The
cross-covariance is a uniform mixing with parameters
\[
    \sigma_1^2 = 0.25,~\sigma_2^2 = 0.6, \gamma_0 = 0.3
\]
and the mean function is a constant one with $\mu_0=(1, -2)$.
\medskip

The plot below shows one realization of the field on the full grid.
\begin{figure}[tbh!p]
\centering
\includegraphics[scale=0.65]{images/sample_low_correlation.png}
\caption{Simulated first (left) and second (right) component of the field.}
\end{figure}

We can also increase the cross-correlation factor $\gamma_0$ to $0.9$ to see
its effect.
\begin{figure}[tbh!p]
\centering
\includegraphics[scale=0.65]{images/sample_high_correlation.png}
\caption{Simulation of highly cross-correlated field.}
\end{figure}

\newpage
The code below shows how simple it is to sample a multivariate GRF using MESLAS.
\usemintedstyle{tango}
\inputminted{python}{example_sample.py}

\section{Co-Kriging}
Cokriging is also implemented in full generality (heterotopic) in MESLAS. The package also provides conditional simulations. Plots below show an example of cokriging and conditional simulation. The GRF is the same as in the previous section, with a high cross-correlation $\gamma_0=0.9$.

\begin{figure}[tbh!p]
\centering
\subfloat[Conditional mean]{\includegraphics[scale=0.6]{images/cond_mean_high_corr.png}}\\
\subfloat[Conditional realisation]{\includegraphics[scale=0.6]{images/cond_realisation_high_corr.png}}
\caption{Example of co-kriging a 2-dimensional GRF with MESLAS, observation locations in red.}
\label{fig:example_inv_prob}
\end{figure}

\newpage

\section{Coverage Function}
We compute the $p$-dimensional CDF using the MVNORM package of Sebastion Marmin. We re-packaged it for streamlined distribution via PiPy. The implementation is $3$ times faster than barebone PyTorch (in dimension 2). It is also vectorized over the batch dimension.

\subsection{Illustrations}
The agreed-upon goal was to produce plots to illustrate multivariate excursion
sets vs univariate.

I suggest we do that by trying to replace figure 4 in the paper.
So: sample unconditionally, illustrate various excursions, krig using this
realisation, plot coverages.

\begin{figure}[tbh!p]
\centering
\subfloat[Simulated Realisation]{\includegraphics[scale=0.6]{images/Figure4/cond_realisation.png}}\\
\subfloat[Cokriging Mean (observation locations in red)]{\includegraphics[scale=0.6]{images/Figure4/cond_mean.png}}\\
\subfloat[Coverage Function]{
	\includegraphics[scale=0.45]{images/Figure4/salinity_excu.png}
	\includegraphics[scale=0.45]{images/Figure4/temperature_excu.png}
	\includegraphics[scale=0.45]{images/Figure4/joint_excu.png}}
\caption{Diagnostics of problems in computation of the coverage function.}
\label{fig:example_inv_prob}
\end{figure}

\newpage
\section{Discrete Setup}

\subsection{Variance Reduction}
The plot clearly demonstrates that the implementation is correct (see the variance reduction of unobserved components due to correlations).

\begin{figure}[tbh!p]
\centering
	\includegraphics[scale=0.8]{images/variance_reduction.png}
\caption{Variance reduction caused by hypothetical measurements at the (generalized) locations in red.}
\end{figure}

\subsection{EIBV Criterion}
The plot below demonstrates computation of the EIBV criterion on the whole design grid. The steps are the following.
\begin{itemize}
  \item First a ground truth is generated by sampling from the GRF model.
  \item Then some data is collected (here 9 data points, following a straight northwards route).
  \item The data is used to compute kriging mean and covariances.
  \item Given those quantities, the EIBV criterion is computed for each point of the design grid, to determine which locations might be interesting for the next data collection steps.
\end{itemize}

The discretization used here is an equilateral tringular grid, with 25 nodes along one dimension (740 nodes in total). The small number of nodes gives rise to artifacts when projecting the observations locations to the grid and when plotting.

\begin{figure}[tbh!p]
\centering
\subfloat[Ground truth (observation locations in red)]{
\includegraphics[scale=0.75]{images/eibv_whole_design/ground_truth.png}}\\
\subfloat[Kriging mean]{
	\includegraphics[scale=0.75]{images/eibv_whole_design/kriging_mean.png}}\\
\subfloat[True excursion, current estimated excursion and EIBV criterion. (red where both excursion conditions are satisfied, white if only one).]{
	\includegraphics[scale=0.5]{images/eibv_whole_design/true_excursion.png}
	\includegraphics[scale=0.5]{images/eibv_whole_design/estimated_excursion.png}
	\includegraphics[scale=0.51]{images/eibv_whole_design/eibv.png}
	}
\caption{Example of computation of EIBV criterion on whole grid.}
\label{fig:example_inv_prob}
\end{figure}

\newpage
\section{Hardware/Software Specifications}
The AUV feature a Nvidia TX1 with ubuntu 16.04. Current programs use Python
2.7, but 3.7 can be used.
In order to run 3.7, we have to run
another backseat-driver (responsible for sending waypoints to the low-leve
controllers).


\end{document}
